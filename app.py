import streamlit as st
import pandas as pd
import requests
import io
from datetime import datetime
import hashlib, hmac, base64, time, json

# ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API ì„¤ì •
NAVER_API_URL = "https://api.naver.com/ncc/keywords"
CUSTOMER_ID = "1806006"
SECRET_KEY = "AQAAAACw74r1xezPDFy7DunyO5PTqpt3IjSZZUVqtxEVMp/33g=="

def get_naver_headers(method, path, body_str):
    timestamp = str(int(time.time() * 1000))
    message = f"{method} {path}\n{timestamp}\n{CUSTOMER_ID}\n{body_str}"
    signature = base64.b64encode(hmac.new(SECRET_KEY.encode('utf-8'), message.encode('utf-8'), hashlib.sha256).digest()).decode('utf-8')
    return {
        "X-Timestamp": timestamp,
        "X-Client-Id": CUSTOMER_ID,
        "X-Client-Secret": SECRET_KEY,
        "X-Signature": signature,
        "Content-Type": "application/json"
    }

@st.cache_data(show_spinner=False)
def fetch_naver_keyword_data(keyword):
    path = "/ncc/keywords"
    body = {
        "hintKeywords": [keyword],
        "range": "MONTHLY",
        "timeRange": "LAST_12_MONTHS"
    }
    res = requests.post(NAVER_API_URL, headers=get_naver_headers("POST", path, json.dumps(body)), json=body, timeout=10)
    if res.status_code == 200:
        return res.json().get("keywordList", [])
    return []

@st.cache_data(show_spinner=False)
def fetch_domaggook_count(keyword):
    res = requests.get(
        "https://domeggook.com/ssl/api/",
        params={
            "ver": "4.0",
            "mode": "getItemList",
            "aid": "097fc5678c5b66bf0b7dd2be8d1b7fdb",
            "market": "dome",
            "keyword": keyword,
            "om": "json"
        },
        timeout=10
    )
    data = res.json()
    return int(data.get("totalCount", 0))

@st.cache_data(show_spinner=False)
def fetch_filtered_keywords(keyword):
    data = fetch_naver_keyword_data(keyword)
    results = []
    for item in data:
        search_sum = item["pcKeyword"]["monthlyAvgSearchAmount"] + item["mobileKeyword"]["monthlyAvgSearchAmount"]
        if search_sum <= 3000 and item["competitor"] == "LOW":
            count = fetch_domaggook_count(item["keyword"])
            if count <= 10000:
                results.append(item["keyword"])
        if len(results) >= 10:
            break
    return results

@st.cache_data(show_spinner=False)
def generate_filtered_names(keyword):
    kws = fetch_filtered_keywords(keyword)
    if not kws:
        return ["ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤"]
    names = []
    for k in kws:
        nm = f"{k} ë¬´ì„  ì´ˆì†Œí˜• ê°•í’ íœ´ëŒ€ìš© ì„ í’ê¸°"
        if len(nm) <= 49:
            names.append(nm)
    return names[:10]

def extract_keyword(title):
    for kw in ["ì†í’ê¸°","ì„ í’ê¸°","ë³´ëƒ‰ë°±","í”¼í¬ë‹‰","ìº í•‘"]:
        if kw in title:
            return kw
    return title.split()[0] if title else ""

st.title("ğŸ“¦ ìë™ ìƒí’ˆëª… ìƒì„±ê¸° (ì‹¤ì‹œê°„ í•„í„°ë§)")
st.markdown("í‚¤ì›Œë“œ ì…ë ¥ ë˜ëŠ” ì—‘ì…€ ì—…ë¡œë“œ â†’ ì¡°ê±´ ë§Œì¡± ì‹œ ì¶”ì²œ ìƒí’ˆëª… 10ê°œ ìƒì„±")

input_kw = st.text_input("ëŒ€í‘œ í‚¤ì›Œë“œ ì…ë ¥ (ì˜ˆ: ì†í’ê¸°)")
if input_kw:
    for nm in generate_filtered_names(input_kw):
        st.write("- ", nm)

uploaded = st.file_uploader("ë˜ëŠ” ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"])
if uploaded:
    df = pd.read_excel(uploaded)
    df.columns = ["ë„ë§¤ì²˜_ìƒí’ˆëª…"]
    df["ëŒ€í‘œí‚¤ì›Œë“œ"] = df["ë„ë§¤ì²˜_ìƒí’ˆëª…"].apply(extract_keyword)
    df["ì¶”ì²œìƒí’ˆëª…"] = df["ëŒ€í‘œí‚¤ì›Œë“œ"].apply(lambda x: "; ".join(generate_filtered_names(x)))
    st.dataframe(df)
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as w:
        df.to_excel(w, index=False, sheet_name="ì¶”ì²œ")
    st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", buf.getvalue(), file_name=f"ì¶”ì²œ_{datetime.today().strftime('%Y%m%d')}.xlsx",
                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
